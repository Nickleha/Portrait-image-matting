{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nick Harris  \n",
    "Fellowship.ai image segmentation challenge  \n",
    "The project will apply a pre-trained segmentation model to the _CelebA_ dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd #easy DF imports\n",
    "import os #path handling\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns #good functionality on top of matplotlib\n",
    "import cv2\n",
    "import shutil\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import tarfile\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "from PIL import Image\n",
    "import helper\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from glob import glob\n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing annotations first to perform EDA in order to see general distribution of various attributes seen in these images. \n",
    "pathbase=r\"C:\\Users\\nickh\\fellowshipAI\" #harcoded because notebook stored in git folder, images/annotations local\n",
    "attr_path=r\"Portrait-image-matting\\celebaHQ\\CelebAMask-HQ-attribute-anno.txt\"\n",
    "celeb_attr=pd.read_csv(os.sep.join([pathbase, attr_path]), sep=\" +\", header=1, engine='python')\n",
    "attr_tidy=celeb_attr.melt(var_name='attribute') #data needs to be tidy for easy graphing with seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(celeb_attr.info())\n",
    "#Just verifying there aren't null entries in attribute dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplots to visualize relative proportion of images that have each attribute\n",
    "sns.catplot(data=attr_tidy, x='value', y=None, col='attribute', kind='count', col_wrap=5, height=3, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that a number of attributes are not going to be well represented in this set. This isn't suprising given that many of these attributes are expected to belong to only a smaller percentage of the population (sadly 50% of people don't sport mustaches), but it does give us cases to scrutinize once we have a working model. Looking at a combination of proportional representation and how that attribute would effect an object's boundary, we can identify a few attributes that may perform poorly: wearing_hat, eyeglasses, mustache and double chin. There are some other attributes such as bald that have low representation, but given that baldnesss should make for more smooth, distinct boundary lines it likely wouldn't create problems in a segmentation model. \n",
    "\n",
    "Racial attributes aren't included in the dataset, but given that this data comes from a set of western celebrities, poor minority representation is likely. A quick manual scroll through the image set confirms this, with largely white subjects visible. This is a problem with most facial datasets I've come across, and it's well documented that this generally leads to worse performance working with BIPOC persons in images. It shouldn't show up heavily in test statistics since they're coming from the same dataset, but would be issue with real world application of the model.\n",
    "   \n",
    "  \n",
    "The creators of the original CelebA dataset also created a subset of their dataset with pixel-wise annotations. We can use this dataset in order to fine-tune a model for person specific image matting. The dataset gives annotations in the form of image maps for each facial attribute, but I'm only concerned about person vs background. Each of these individual maps will need to be merged into one person vs background label for training/validation/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_images = 30000 #total number of images in the HQ dataset\n",
    "#list of all attributes mapped in the dataset. All of these will become a 1 for person in the mask.\n",
    "labels = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye', 'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth', 'u_lip', 'l_lip', 'hair', 'hat', 'ear_r', 'neck_l', 'neck', 'cloth']\n",
    "source_folder = r\"C:\\Users\\nickh\\fellowshipAI\\Portrait-image-matting\\celebaHQ\\CelebAMask-HQ-mask-anno\" #annotations folder\n",
    "dest_folder = r\"C:\\Users\\nickh\\fellowshipAI\\Portrait-image-matting\\celebaHQ\\SegmentationClass\"  #final mask folder\n",
    "for k in range(tot_images):\n",
    "    folder_num = k // 2000 #dataset split into 15 folders with each containing annotations for 2000 images split in numerical order\n",
    "    mask = np.zeros((512, 512)) #dataset is already resized/cropped into 512x512 images. \n",
    "    for i, label in enumerate(labels):\n",
    "        filename = os.path.join(source_folder, str(folder_num), str(k).rjust(5, '0') + '_' + label + '.png')\n",
    "        if (os.path.exists(filename)):\n",
    "            img_temp = cv2.imread(filename)\n",
    "            img_temp = img_temp[:, :, 0]\n",
    "            #labels any pixel in an attribute map as non-background. \n",
    "            mask[img_temp != 0] = (1) \n",
    "\n",
    "    filename_mask = os.path.join(dest_folder, str(k) + '.png')\n",
    "    cv2.imwrite(filename_mask, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have converted the individual attribute masks for each image into a single person vs background mask for each image. The original celeba dataset includes a list_eval partition, and the makers of the HQ dataset recommend using the same partition. The datasets are different sizes (nobody wants to manually annotate 200k images), so they've supplied a mapping file to allow the subset of images used for the HQ dataset to be used on the orginal test/eval/train partition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source does not state relative proportions of train/test/val for HQ dataset\n",
    "#Will validate we have reasonable proportions after separation\n",
    "train_count = 0\n",
    "test_count = 0\n",
    "val_count = 0\n",
    "\n",
    "#Source folders\n",
    "img_source = r\"celebaHQ\\JPEGImages\"\n",
    "mask_source = r\"celebaHQ\\SegmentationClass\"\n",
    "#Training Dest. folders\n",
    "img_dest_train = r\"C:\\Users\\nickh\\fellowshipAI\\Portrait-image-matting\\celebaHQ\\splits\\train_img\"\n",
    "mask_dest_train = r\"C:\\Users\\nickh\\fellowshipAI\\Portrait-image-matting\\celebaHQ\\splits\\train_img\\label\"\n",
    "#Validation Dest. folders\n",
    "img_dest_val = r\"celebaHQ\\splits\\val_img\"\n",
    "mask_dest_val = r\"celebaHQ\\splits\\val_img\\label\"\n",
    "#Test Dest. folders\n",
    "img_dest_test = r\"celebaHQ\\splits\\test_img\"\n",
    "mask_dest_test = r\"celebaHQ\\splits\\test_img\\label\"\n",
    "\n",
    "image_mapping = pd.read_csv(r\"celebaHQ\\CelebA-HQ-to-CelebA-mapping.txt\", delim_whitespace=True, header=None)\n",
    "\n",
    "for i, x in enumerate(image_mapping.loc[:, 1]):\n",
    "    if x < 162771:\n",
    "        shutil.copy2(os.path.join(mask_source, str(i)+'.png'), os.path.join(mask_dest_train, str(val_count)+'.png'))\n",
    "        shutil.copy2(os.path.join(img_source, str(i)+'.jpg'), os.path.join(img_dest_train, str(val_count)+'.jpg')) \n",
    "        train_count += 1  \n",
    "        \n",
    "    elif  x >= 182638:\n",
    "        shutil.copy2(os.path.join(mask_source, str(i)+'.png'), os.path.join(mask_dest_test, str(val_count)+'.png'))\n",
    "        shutil.copy2(os.path.join(img_source, str(i)+'.jpg'), os.path.join(img_dest_test, str(val_count)+'.jpg')) \n",
    "        test_count += 1 \n",
    "        \n",
    "    else:\n",
    "        shutil.copy2(os.path.join(mask_source, str(i)+'.png'), os.path.join(mask_dest_val, str(val_count)+'.png'))\n",
    "        shutil.copy2(os.path.join(img_source, str(i)+'.jpg'), os.path.join(img_dest_val, str(val_count)+'.jpg'))        \n",
    "        val_count += 1\n",
    "        \n",
    "\n",
    "total_counts = train_count + test_count + val_count\n",
    "print('training portion is ' + str(train_count/total_counts) + '   Counts are:' + str(train_count))\n",
    "print('validation portion is ' + str(val_count/total_counts) + '   Counts are:' + str(val_count))\n",
    "print('test portion is ' + str(test_count/total_counts) + '   Counts are:' + str(test_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CelebA-HQ training/validation/test splits are set up for a roughly 80/10/10 split, which is reasonable and commonly used. With some basic EDA done and everything set up on our local environment, it's time to choose a model.   \n",
    "\n",
    "The dataset with avilable mask annotations for training is also already cropped and resized to a fixed input (512x512). For the paper associated with the creation of this dataset, the backbone is an FCN-8 architecture[1]. I will create my own and train it. In order to reduce total training time, I will use pre-trained weights for VGG-16 taken from tensorflow stored models, trained on imagenet. \n",
    "\n",
    "In general, any FCN architecture would work for this task. The FCN-16 and FCN-32 architectures would have lower accuracy due to the large stride needed when upsampling directly from the last pool layer. In general, both FCN-16 and FCN-32 will generate too coarse of a map.\n",
    "\n",
    "FCN-8 can be made using the first layers of a pre-trained VGG-16 model. We should see higher initial skill and a faster skill improvement using pre-trained weights from a VGG-16 model as the encoder. The models will have to diverge once the VGG-16 architecture gets to the fully connected layers. The pre-trained weights from the original VGG-16 model can still be used though, and should be extracted. I will build in tensorflow syntax rather than keras in order to get some experience on the backend that's covered with the Keras api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation will start by taking layers from VGG-16\n",
    "#There is already vgg16 model and pre-trained weights in my local working directory\n",
    "def load_vgg_16(sess, vgg_model_path):\n",
    "    model = tf.saved_model.loader.load(sess, export_dir = vgg_model_path, tags=['vgg16'])\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    image_in = graph.get_tensor_by_name('image_input:0')\n",
    "    keep_per = graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #3,4,7 are specifically needed as they are used to create skip-layers used in the upsampling process\n",
    "    #L_7 is the last stage before this model deviates from VGG-16 architecture\n",
    "    L_3 = graph.get_tensor_by_name('layer3_out:0')\n",
    "    L_4 = graph.get_tensor_by_name('layer4_out:0')\n",
    "    L_7 = graph.get_tensor_by_name('layer7_out:0')\n",
    "    \n",
    "    return image_in, keep_per, L_3, L_4, L_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to create the remaining layers.\n",
    "#When creating this, I was envisioning only segmenting the foreground and background. Therefore, I would only use 2 classes.\n",
    "def layer_create(L_3_out, L_4_out, L_7_out, num_classes = 2):\n",
    "\n",
    "    # Would be the first dense layer in VGG architecture. 1x1 convolution for FCN-8. Happily, i get to name my first layer fcn8\n",
    "    fcn_8 = tf.layers.conv2d(L_7_out, filters=num_classes, kernel_size=1, name=\"fcn_8\")\n",
    "\n",
    "    # Upsampling to match layer 4. FCN-8 skip architecture connects this layer to L_4\n",
    "    fcn_9 = tf.layers.conv2d_transpose(fcn_8, filters=L_4_out.get_shape().as_list()[-1],\n",
    "    kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn_9\")\n",
    "\n",
    "    #Connecting FCN_8 to layer 4 for skip connection\n",
    "    fcn_9_skip = tf.add(fcn_9, L_4_out, name=\"fcn_9_plus_L_4_out\")\n",
    "\n",
    "    # Upsample\n",
    "    fcn_10 = tf.layers.conv2d_transpose(fcn_9_skip, filters=L_3_out.get_shape().as_list()[-1],\n",
    "    kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn_10\")\n",
    "\n",
    "    # Add skip connection\n",
    "    fcn_10_skip = tf.add(fcn_10, L_3_out, name=\"fcn_10_plus_L_3_out\")\n",
    "\n",
    "    # Upsample again. 8x8 stride, hence the name\n",
    "    fcn_11 = tf.layers.conv2d_transpose(fcn_10_skip, filters=num_classes,\n",
    "    kernel_size=16, strides=(8, 8), padding='SAME', name=\"fcn_11\")\n",
    "\n",
    "    return fcn_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining our optimizer and loss function\n",
    "def optimize(fcn_last_layer, correct_label, learn_rate, num_classes = 2):\n",
    "  \n",
    "    # 4D tensors to 2D for calculations, row=pixel, column=class\n",
    "    logits = tf.reshape(fcn_last_layer, (-1, num_classes), name=\"fcn_logits\")\n",
    "    correct_label_rs = tf.reshape(correct_label, (-1, num_classes))\n",
    "\n",
    "    # Distance from actual labels from cross\n",
    "    dist = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=correct_label_rs[:])\n",
    "    loss_op = tf.reduce_mean(dist, name=\"fcn_loss\")\n",
    "\n",
    "    # Using adams optimizer as the optimization algorithm. It has some slight benefits over stochastic-GD. \n",
    "    #Incorporates learning rates applied per parameter ( similar to Adagrad) and recent gradient magnitures (like RMSProp)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learn_rate).minimize(loss_op, name=\"fcn_train_op\")\n",
    "\n",
    "    return logits, train_op, loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform mean_iou on test set per run\n",
    "def mean_iou(sess, logits, keep_prob, image_pl, data_folder, image_shape):\n",
    "    image_files = glob(os.path.join(data_folder, '*.jpg'))\n",
    "    label_paths = glob(os.path.join(data_folder, 'label','*.png'))\n",
    "    miou_store = []\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        gt_image_file = label_paths[i]\n",
    "        temp_gt = cv2.imread(gt_image_file)\n",
    "        # Re-size to image_shape\n",
    "        gt_image = scipy.misc.imresize(temp_gt, image_shape)\n",
    "        gt_image = tf.reshape(correct_label, (-1, num_classes))\n",
    "        image = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
    "        # Run inference\n",
    "        im_softmax = sess.run([tf.nn.softmax(logits)],{keep_prob: 1.0, image_pl: [image]})\n",
    "        # Splice out second column (person), reshape output back to image_shape\n",
    "        im_softmax = im_softmax[0][:, 1].reshape(image_shape[0], image_shape[1])\n",
    "        # If person softmax > 0.5, prediction is person\n",
    "        segmentation = (im_softmax > 0.5).reshape(image_shape[0], image_shape[1], 1)\n",
    "\n",
    "        #print out mIoU for this run on test set\n",
    "        miou, _ = tf.compat.v1.metrics.mean_iou(gt_image, segmentation, 2)\n",
    "        miou_store.append(miou)\n",
    "    return miou_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def train_fcn(sess, epochs, batch_size, get_batches, train_op, cross_entropy_loss, \n",
    "              input_image,correct_label, keep_prob, learning_rate):\n",
    "    keep_prob_value = 0.5\n",
    "    learning_rate_value = 0.001\n",
    "    for epoch in range(epochs):\n",
    "        # Create function to get batches\n",
    "        total_loss = 0\n",
    "        for X_batch, gt_batch in get_batches(batch_size):\n",
    "\n",
    "            loss, _ = sess.run([cross_entropy_loss, train_op],\n",
    "            feed_dict={input_image: X_batch, correct_label: gt_batch,\n",
    "            keep_prob: keep_prob_value, learning_rate:learning_rate_value})\n",
    "\n",
    "            total_loss += loss;\n",
    "\n",
    "        print(\"EPOCH {} ...\".format(epoch + 1))\n",
    "        print(\"Loss = {:.3f}\".format(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    # Download pretrained vgg model\n",
    "    helper.maybe_download_pretrained_vgg(data_dir)\n",
    "\n",
    "    # batches\n",
    "    get_batches_fn = helper.gen_batch_function(training_dir, image_shape)\n",
    "  \n",
    "    with tf.Session() as session:\n",
    "        \n",
    "        # Returns the three layers, keep probability and input layer from the vgg architecture\n",
    "        image_input, keep_prob, layer3, layer4, layer7 = load_vgg_16(session, vgg_model_path)\n",
    "\n",
    "        # The resulting network architecture from adding a decoder on top of the given vgg model\n",
    "        model_output = layer_create(layer3, layer4, layer7, num_classes)\n",
    "\n",
    "        logits, train_op, cross_entropy_loss = optimize(model_output, correct_label, learning_rate, num_classes)\n",
    "    \n",
    "        # Initialize all variables\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.local_variables_initializer())\n",
    "\n",
    "        print(\"Model build successful, starting training\")\n",
    "\n",
    "        # Train the neural network\n",
    "        train_fcn(session, EPOCHS, BATCH_SIZE, get_batches_fn, \n",
    "             train_op, cross_entropy_loss, image_input,\n",
    "             correct_label, keep_prob, learning_rate)\n",
    "\n",
    "        # Run the model with the test images for visualization purposes/manual validation that model is working as intended\n",
    "        helper.save_inference_samples(runs_dir, data_dir, session, image_shape, logits, keep_prob, image_input)\n",
    "        \n",
    "        #Calculate mean IoU for this model on test set\n",
    "        m_iou = mean_iou(session, logits, keep_prob, image_input, test_path, image_shape)\n",
    "        print(m_iou)\n",
    "       \n",
    "        #saving model\n",
    "        saved_model = tf.train.Saver()\n",
    "        #unique modelname for keeping track of saved models by tuning param\n",
    "        modelname= ('fcn8-' + str(EPOCHS) + 'epochs-' + str(BATCH_SIZE) + 'batch-' + (str(DROPOUT)[-2:]) + 'dropout')\n",
    "        saved_model.save(session, modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning parameters\n",
    "num_classes = 2\n",
    "#helper function resizes. Images are 1024x 1024. Maps are 512x 512. \n",
    "#Dropping to 128x 128 for speed due to unfortunate circumstances forcing local training on a laptop.\n",
    "#Training on small subset of dataset due to lack of gpu/access to kaggle\n",
    "image_shape = (128, 128)\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "DROPOUT = 0.9\n",
    "\n",
    "\n",
    "#Directory Paths\n",
    "data_dir = r\"C:\\Users\\nickh\\fellowshipAI\\Portrait-image-matting\\celebaHQ\"\n",
    "runs_dir = './runs'\n",
    "training_dir ='./celebaHQ/splits/train_small'\n",
    "vgg_model_path = r\"C:\\Users\\nickh\\fellowshipAI\\Portrait-image-matting\\celebaHQ\\splits\\vgg\"\n",
    "test_path = r\"C:\\Users\\nickh\\fellowshipAI\\Portrait-image-matting\\celebaHQ\\splits\\test_small\"\n",
    "\n",
    "#initialize tensors\n",
    "correct_label = tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], num_classes], name='label_pl')\n",
    "learning_rate = tf.placeholder(tf.float32, name='learnrate_pl')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob_pl')\n",
    "image_pl = tf.placeholder(tf.float32, name='image_pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could use pixel-wise accuracy to evaluate the model (our images are relatively well balanced in terms of class percentages in each image), it's generally a better practice to use mean Intercept over Union (mIoU). When a class has no positive values within an image, mIoU may be a poor metric. This problem involves only 2 classes that are relatively well spread, so it will be a good metric to use. \n",
    "   \n",
    "After a model runs, the helper script contains a gen_test_output function. This function will output both a mean IoU on the test set and some example images to a folder in the current directory for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-1b2a4a534d52>:4: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\nickh\\fellowshipAI\\Portrait-image-matting\\celebaHQ\\splits\\vgg\\variables\\variables\n",
      "WARNING:tensorflow:From <ipython-input-4-e55d0f434aab>:6: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-e55d0f434aab>:10: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d_transpose instead.\n",
      "Model build successful, starting training\n",
      "EPOCH 1 ...\n",
      "Loss = 154.715\n",
      "Training Finished. Saving test images to: ./runs\\1595299595.1562564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-30c6a545bcc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#will run on multiple sets of tuning parameters, with each model exported by name generator at end of run_training fcn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'helper.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-27f07683bdef>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m#Calculate mean IoU for this model on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mm_iou\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_iou\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_iou\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-66b079d1670c>\u001b[0m in \u001b[0;36mmean_iou\u001b[1;34m(sess, logits, keep_prob, image_pl, data_folder, image_shape)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#print out mIoU for this run on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mmiou\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_iou\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mmiou_store\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmiou\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmiou_store\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py\u001b[0m in \u001b[0;36mmean_iou\u001b[1;34m(labels, predictions, num_classes, weights, metrics_collections, updates_collections, name)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                                      (predictions, labels, weights)):\n\u001b[0;32m   1130\u001b[0m     \u001b[1;31m# Check if shape is compatible.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m     \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     total_cm, update_op = _streaming_confusion_matrix(labels, predictions,\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "#Repurposed helper file to download model model, batch generator functions\n",
    "#will run on multiple sets of tuning parameters, with each model exported by name generator at end of run_training fcn\n",
    "%run helper.py\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few different iterations of the tuning parameters were tested. Batch size was left constant, as anything above 4 ran into memory errors and I didn't want to reduce image dimensions further due to detail loss. Dropout ranges from 0.25 to 0.9. Number of epochs trained ranged from 3 (for unlikely to work settings, such as 0.9 dropout) to 10(30 or 50 should be used without time constraints, loss was still decreasing at 10). The best results were obtained using a standard dropout. Lower than normal dropouts also performed very well, likely due to the limited dataset size. Dropout is generally used to reduce overfitting, and the very small dataset size used (<300 images in training set) meant that the model was very prone to overfitting. In this case, the longer training times caused by high dropout rate were well worth it. Learning rate was also tested at 0.1, 0.01 and 0.001, with the lowest learning rate appearing to show the lowest loss after 10 epochs.    \n",
    "\n",
    "Current SoTA for mIoU image segmentation on portraits is around 0.98 using the architecture and feature engineering used by Shen et. al for their automatic segmentation [1]. In the same paper, an FCN-8 with minimual tuning had results ranging from 0.77 to 0.85. Although my model did not hit that benchmark, it is likely that the mIoU could be increased to those levels with a few simple changes. If implemented in an cloud GPU system, we'd be able to train on much more larger dataset. With ~24000 images available in my training set and a higher resolution of 512x512, it is likely we'd be able to come close to current benchmarks.   \n",
    "\n",
    "When remaking this problem, I would likely use a more complex architecture. A pre-trained version of DeepLab v3+ with fine-tuning performed on only the top layers of the model would be able to easily beat the mIoU obtained by this FCN-8, regardless of how much I increase the training set or resolution. There are also some interesting opportunities for feature engineering given the type of images seen in this set. With self portrait images(selfies), relative position to facial center is an important factor. Using a facial feature detector on image preprocessing combined with giving each pixel a normalized difference from facial center would be a good way to capture that information, though it would remove the possiblity of doing incremental training and require full model fine-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References  \n",
    "\n",
    "[1]Xiaoyong Shen, Aaron Hertzmann, Jiaya Jia, Sylvain Paris, Brian Price, Eli Shechtman, and Ian Sachs. 2016. Automatic portrait segmentation for image stylization. In Proceedings of the 37th Annual Conference of the European Association for Computer Graphics (EG ’16). Eurographics Association, Goslar, DEU, 93–102."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
